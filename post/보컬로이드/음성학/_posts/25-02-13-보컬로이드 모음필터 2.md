---
layout: post
date: 2025-02-13 17:26 +09:00
tags:
  - 음성학
  - 보컬로이드
---
# 음원 형식

![](/media/Pasted image 20240116164228.png)

위 사진은 120bpm에서 찍었다.

각 음은 약 6초의 길이를 가지며, 2초부터 4초까지 PB가 최소값에서 최대값으로 올라간다. 이때 PBS는 12로 두어 피치가 옥타브를 커버하게 만들었다. 또한 PB가 변화하는 구간 양옆으로 0.5초간의 마진을 두었다.
앞에 1.5초간의 불필요한 구간이 생기는데, 어택으로 인한 볼륨 변화 때문에 어쩔 수 없이 버려야 했다.

한 음당 6초이므로 한 옥타브에 72초 분량의 음원이 필요하다.

# 코드

python을 사용했으며, librosa와 numpy, pyplot, pandas 등을 이용했다.

## 임포트

{% raw %}
```python
import numpy as np
import librosa
import matplotlib.pyplot as plt
import pandas as pd
import colorsys
```
{% endraw %}

## FFT
{% raw %}
```python
def fftData(wave):
	global sample_rate

	fft = np.fft.fft(wave)
	spectrum = np.abs(fft) * 2 / len(wave)
	left_spectrum = spectrum[:int(len(spectrum)/2)]

	fdomain = np.fft.fftfreq(len(wave), d=1/sample_rate)
	left_fdomain = fdomain[:int(len(spectrum)/2)]

	return left_spectrum, left_fdomain
```
{% endraw %}

푸리에 변환을 거친 데이터의 y값이 명확치가 않은데, 복소수 타입이기 때문에 정제가 필요하다. (`spectrum = np.abs(fft) * 2 / len(wave)`) 일반적으로 진폭(amplitude)으로 변환하고자 위와 같이 계산한다. 이 외에도 Power, dB 등으로 변환할 수도 있다. [참고](https://ballpen.blog/fft-%EB%B6%84%EC%84%9D-%EC%98%88%EC%8B%9C/).

## YIN

음성의 배음열은 등간격이기 때문에 기음의 주파수만 알고 있다면 배음열의 주파수들을 모두 알 수 있다. 그리고 음원의 기음은 계속 변화하기 때문에 음원으 음을 시계열로 나타내주는 YIN 알고리즘을 사용한다.

{% raw %}
```python
def vowelFilter(wave):
	global sample_rate

	yinframe = 1024
	ypitch = librosa.yin(wave, fmin=110, fmax=1760,
		sr=sample_rate,
		frame_length=yinframe*2,
		win_length=yinframe,
		hop_length=yinframe
	)[1:]
	
    # ...
```
{% endraw %}

`win_length`랑 `hop_length`는 시계열의 시간 단위 크기를 정하는 파라미터다. 정확힌 샘플의 개수로 정해진다.
`win_length`는 한 번 분석할 때 사용하는 샘플 크기다. 그러나 보통 이전 `win_length`가 끝난 바로 다음부터 `win_length`만큼 새로 분석하진 않고, 이전 시작점에서 `hop_length`만큼 지난 지점에서부터 새로 `win_length`를 분석한단다.  따라서 각 분석마다 overlap이 발생한다. overlap 크기는 `win_length - hop_length`와 같다. [참고](https://towardsdatascience.com/all-you-need-to-know-to-start-speech-processing-with-deep-learning-102c916edf62).
그러나 지금 상황에서 overlap은 필요하지 않다고 판단하여 `win_length`와 `hop_length`를 똑같은 수치로 맞춰놓았다.

여튼 이러고 나면 대충 아래같은 데이터가 생긴다. 이 개수는 대충 `len(wave) / win_length`와 같다.


{% raw %}
```
array([251.11394126, 249.18441147, 252.91401519, 256.0604 , 260.76881408, 265.27914282, ...
```
{% endraw %}

그리고 여기서 첫번째 데이터는 제외시키는데, 값이 좀 요상하게 나와서 그렇다.

## 배음열 구하기

기음을 구하였으므로 배음열의 위치를 구할 수 있고, 각 위치에서의 크기(amplitude)를 FFT 결과에서 구하면 된다. 

그러나 생각보다 쉽지 않았다.

### Truncation 문제

FFT를 거쳤을 때 나타나는 주파수 도메인은 해상도를 가진다. 이는 샘플의 길이와 반비례한다. 만약 샘플 길이가 2초라면 FFT를 거친 주파수 도메인은 0.5hz 단위로 나타나게 된다.

그런데 만약 0.5hz 사이에 배음이 위치한다면 어떻게 될까. 그래프 상에서 극점이 짤리는 문제가 발생한다. 

![](/media/Pasted image 20240116012414.png)

직관적인 방법으론 양쪽의 기울기를 이용해 극점을 추정하는 방법이 있다. 

![](/media/Pasted image 20240116012515.png)

하지만 어디까지나 근사적인 방법일 뿐, 생각보다 정확하지 않고 오차가 컸다.

또 다른 방법으론 샘플 길이를 늘려 주파수의 해상도를 높이는 방법도 있지만, 이것도 한계가 있다.

### 주파수 해상도 조정

고민을 거듭하다, 주파수 해상도를 배음열의 간격과 맞추도록 두었다. 만약 A4 (440hz)를 분석한다면, 주파수 간격을 줄일 게 아니라 440hz로 맞춰버리는 것이다.

![](/media/Pasted image 20240116172518.png)

이러한 방식으로 249.18hz 지점을 분석한 결과로, ratio는 freq과 근음 249.18hz의 비를 계산한 것이다. 보다시피 약간의 오차가 있다.

주파수의 해상도는 분석하는 음원의 길이에 반비례한다. 만약 길이가 0.5초라면 주파수는 2hz단위로 나타나게 된다. 다만 디지털이기 때문에 sample rate를 고려하여 샘플의 크기를 기준으로 계산해야 한다. 
길이가 짧을수록 샘플 하나의 길이가 상대적으로 커져 오차가 발생하게 된다. sample rate 44100hz에서 0.004초면 샘플 176개로, 실제론 정확히 250hz여야 하지만 샘플 개수로 따지면 약 250.57hz로 약간 차이가 발생한다.

때문에 근음이 높아질수록, 그리고 고음역대의 배음열일수록 오차가 커지게 된다.

다만 어느 정도는 감안할 수 있는 오차고, 또 해결할 수도 있다. 음원의 sample rate를 높여버리면 오차도 충분히 줄어들어 해결될 것으로 보인다.

앞서 yin으로 구한 각 피치를 근음으로 잡아 배음열의 크기를 구하면 아래와 같다.

{% raw %}
```python
def vowelFilter(wave):
	# ...
	
	result = pd.DataFrame(columns=["root", "freq", "amplitude"])

	startIdx = 0
	for i in range(len(ypitch)):
		pitchFreq = ypitch[i]
		unitLength = 1 / pitchFreq
		sampleCount = int(sample_rate * unitLength)
		startIdx = i * yinframe
		endIdx = startIdx + sampleCount
		unitWave = srcwave[startIdx:endIdx]

		yfft, xfft = fftData(unitWave)
		result = pd.concat([result, pd.DataFrame({'root': pitchFreq,'freq':xfft, 'amplitude':yfft})])

	# ...
		
```
{% endraw %}

배음열 크기를 근음끼리 묶어 보면 대충 이런 모습이 되는데 (다듬기 과정을 거쳤음)

![](/media/Pasted image 20240116173659.png)

이걸 몽땅 몰아넣고 주파수 순으로 다시 정렬하면 모음필터가 그려진다

![](/media/Pasted image 20240116173828.png)

## 다듬기

필요한 주파수 (20~5000)만 잘라내고 dB 단위로 바꾼다.

{% raw %}
```python
def vowelFilter(wave):
	#...
	
	result = result.loc[ result['freq'] > 20 ]
	result = result.loc[ result['freq'] < 5000 ]
	result = result.sort_values('freq', ignore_index=True)
	result['dB'] = librosa.amplitude_to_db(result['amplitude'])

	return result
```
{% endraw %}
그리고 뾰족뾰족 튀어나온 점들을 정리하기 위해 단위 구간의 평균, 표준편차로 나타내본다.
{% raw %}
```python
def smoothGraph(df):
	smoothResult = pd.DataFrame(columns=["freq", "dB", "mindB", "maxdB"])
	unitfreq = 50
	critvalue = 1.96
	
	i = 0
	while( (i+1)*unitfreq < 5000 ):
		unitStartFreq = i * unitfreq
		unitEndFreq = (i+1) * unitfreq
		unitFiltered = df.loc[ unitStartFreq < df['freq'] ].loc[ df['freq'] < unitEndFreq ]

		meanfreq = np.mean(unitFiltered['freq'])
		meandB = np.mean(unitFiltered['dB'])
		stddB = np.std(unitFiltered['dB'])
		mindB = meandB - critvalue * stddB
		maxdB = meandB + critvalue * stddB
		
		smoothResult.loc[ len(smoothResult) ] = [meanfreq, meandB, mindB, maxdB]
		i += 1

	return smoothResult
```
{% endraw %}
![](/media/Pasted image 20240116174336.png)

# 참고

- [https://ballpen.blog/fft-%EB%B6%84%EC%84%9D-%EC%98%88%EC%8B%9C/](https://ballpen.blog/fft-%EB%B6%84%EC%84%9D-%EC%98%88%EC%8B%9C/)
- [https://towardsdatascience.com/all-you-need-to-know-to-start-speech-processing-with-deep-learning-102c916edf62/](https://towardsdatascience.com/all-you-need-to-know-to-start-speech-processing-with-deep-learning-102c916edf62/)